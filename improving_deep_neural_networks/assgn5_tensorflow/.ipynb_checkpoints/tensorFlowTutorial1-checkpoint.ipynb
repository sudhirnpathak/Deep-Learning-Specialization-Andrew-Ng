{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Tutorial 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will learn a deep learning framework, TensorFlow. We will learn about the basic structure of a tensorflow program.\n",
    "\n",
    "To learn TensorFlow, let's consider a motivating problem, where we want to minimize the cost function $J(w) = w^2-10w+25$, which is basically $J(w) = (w-5)^2$. So the value of $w$ that minimizes $J(w)$ is $w=5$.\n",
    "\n",
    "Say we did not know the value of $w$ that minimizes $J(w)$. Let's see how we can implement it in TensorFlow to minimize the cost function. A very similar structure of program can be used to train neural network where we can have complicated cost function $J(W,b)$, and we can use TensorFlow to find the parameters $W,b$ that minimizes the cost function $J(W,b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the parameter $w$ and the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(0, dtype = tf.float32)                    # initilizes the variable w to 0\n",
    "cost = tf.add(tf.add(w**2, tf.multiply(-10.,w)), 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define train to be our learning algorith, which uses a gradient descent optimizer, to minimize the cost function. We take learning rate to be 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "session = tf.Session()                     # starts a tensorflow session\n",
    "session.run(init)                          # initilizes global variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tensorflow to evaluate a variable, we run it as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have not run our learning algorithm ('train') yet, $w$ is evaluated as its initial value which is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets run one step of gradient descent, and evaluate the value of $w$ after one step of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.099999994\n"
     ]
    }
   ],
   "source": [
    "session.run(train)\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run 1000 iterations of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9999886\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    session.run(train)\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 1000 iterations of gradient descent, we have found the optimum value of $w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we only defined the cost function which we want to minimize. TensorFlow automatically knows how to take derivative wrt add and multiply as well as other funtions, which is why we just need to implement forward propagation, and it could figure out how to do the gradient calculations of back propagation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
